

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>GMM &mdash; MiscBeginnerSC 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Misc" href="Notes.html" />
    <link rel="prev" title="Fun Math" href="Fun%20Math.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MiscBeginnerSC
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Misc</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Backprop.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bokeh.html">Bokeh</a></li>
<li class="toctree-l1"><a class="reference internal" href="CPP.html">CPP – Notes on The Cherno C++ Videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS%20and%20Algo.html">DS and Algo</a></li>
<li class="toctree-l1"><a class="reference internal" href="Fun%20Math.html">Fun Math</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GMM</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#EM-Algorithm-for-Gaussian-Mixture">EM Algorithm for Gaussian Mixture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#When-Z-Can-Be-Observed">When <span class="math notranslate nohighlight">\(Z\)</span> Can Be Observed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#When-Z-Is-a-Latent-Random-Variable">When <span class="math notranslate nohighlight">\(Z\)</span> Is a Latent Random Variable</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Mixture-Density-Networks-(MDN)">Mixture Density Networks (MDN)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Notes.html">Misc</a></li>
<li class="toctree-l1"><a class="reference internal" href="Piano.html">Piano</a></li>
<li class="toctree-l1"><a class="reference internal" href="Questions_c.html">Question_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="Questions_m.html">Questions_m</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory.html">Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="Untitled.html">Not All LeetCode Questions are Created Equal</a></li>
<li class="toctree-l1"><a class="reference internal" href="Untitled.html#Gauss-Quadrature">Gauss Quadrature</a></li>
<li class="toctree-l1"><a class="reference internal" href="LC/19.html">19</a></li>
<li class="toctree-l1"><a class="reference internal" href="LC/CPP.html">CPP</a></li>
<li class="toctree-l1"><a class="reference internal" href="LC/DP.html">DP</a></li>
<li class="toctree-l1"><a class="reference internal" href="LC/Misc.html">Misc</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiscBeginnerSC</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>GMM</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/GMM.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="GMM">
<h1>GMM<a class="headerlink" href="#GMM" title="Permalink to this headline">¶</a></h1>
<div class="section" id="EM-Algorithm-for-Gaussian-Mixture">
<h2>EM Algorithm for Gaussian Mixture<a class="headerlink" href="#EM-Algorithm-for-Gaussian-Mixture" title="Permalink to this headline">¶</a></h2>
<p>The goal is to estimate a three-component Gaussian mixture model:</p>
<p><span class="math">\begin{eqnarray*}
P(Z = j) &=& \phi_j, \quad j=1, 2, 3\\
(X|Z=j) &\sim& N(\mu_j, \sigma_j).
\end{eqnarray*}</span></p>
<div class="section" id="When-Z-Can-Be-Observed">
<h3>When <span class="math notranslate nohighlight">\(Z\)</span> Can Be Observed<a class="headerlink" href="#When-Z-Can-Be-Observed" title="Permalink to this headline">¶</a></h3>
<p>Denote by <span class="math notranslate nohighlight">\(p(x; \mu, \sigma)\)</span> the probability density function of a normal distribution</p>
<div class="math notranslate nohighlight">
\[p(x; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}.\]</div>
<p>If the sample of <span class="math notranslate nohighlight">\(Z\)</span> is visible, the so called complete data likelihood of one single sample point <span class="math notranslate nohighlight">\((x_i, z_i)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\phi_1p(x_i; \mu_1, \sigma_1)1_{\{z_i = 1\}} + \phi_2p(x_i; \mu_2, \sigma_2)1_{\{z_i = 2\}} + \phi_3p(x_i; \mu_3, \sigma_3)1_{\{z_i = 3\}}.\]</div>
<p>Note that this is different from the actual density function of the Gaussian mixture distribution because of those indicator functions. Although this formula has three terms, the indicator functions ensure that only one will be nonzero, and that the log-likelihood is</p>
<div class="math notranslate nohighlight">
\[\log(\phi_1 p(x_i; \mu_1, \sigma_1))1_{\{z_i = 1\}} + \log(\phi_2 p(x_i; \mu_2, \sigma_2))1_{\{z_i = 2\}} + \log(\phi_3 p(x_i; \mu_3, \sigma_3))1_{\{z_i = 3\}}.\]</div>
<p>Here the indicator functions are NOT in the logarithm and again, only one term will be nonzero.</p>
<p>Given <span class="math notranslate nohighlight">\(m\)</span> sample points <span class="math notranslate nohighlight">\((x_1, z_1), (x_2, z_2), \ldots, (x_m, z_m)\)</span>, the log-likelihood is thus</p>
<p><span class="math">\begin{eqnarray*}
&&\sum_{i=1}^m \left(\log(p(x_i; \mu_1, \sigma_1))1_{\{z_i = 1\}} + \log(p(x_i; \mu_2, \sigma_2))1_{\{z_i = 2\}} + \log(p(x_i; \mu_3, \sigma_3))1_{\{z_i = 3\}} \right)\\
%&=& \sum_{i=1}^m \left[\left( -\log(\sigma_1\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_1}{\sigma_1}\right)^2  \right) 1_{\{z_i = 1\}} +
%                         \left( -\log(\sigma_2\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_2}{\sigma_2}\right)^2  \right)1_{\{z_i = 2\}} +
%                         \left( -\log(\sigma_3\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_3}{\sigma_3}\right)^2  \right)1_{\{z_i = 3\}} \right]\\
%&=& \sum_{1\leq i \leq m, z_i=1} \left( -\log(\sigma_1\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_1}{\sigma_1}\right)^2  \right)
%    + \sum_{1\leq i \leq m, z_i=2} \left( -\log(\sigma_2\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_2}{\sigma_2}\right)^2  \right)
%    + \sum_{1\leq i \leq m, z_i=3} \left( -\log(\sigma_3\sqrt{2\pi}) -\frac{1}{2}\left(\frac{x_i-\mu_3}{\sigma_3}\right)^2  \right)
&=& \sum_{1\leq i \leq m, z_i=1} \log(\phi_1 p(x_i; \mu_1, \sigma_1)) + \sum_{1\leq i \leq m, z_i=2} \log(\phi_2 p(x_i; \mu_2, \sigma_2)) + \sum_{1\leq i \leq m, z_i=3} \log(\phi_3 p(x_i; \mu_3, \sigma_3))\\
&=& \left(\sum_{1\leq i \leq m, z_i=1} \log(p(x_i; \mu_1, \sigma_1)) + \sum_{1\leq i \leq m, z_i=2} \log(p(x_i; \mu_2, \sigma_2)) + \sum_{1\leq i \leq m, z_i=3} \log(p(x_i; \mu_3, \sigma_3))\right) + m_1\log\phi_1 + m_2\log\phi_2 + m_3\log\phi_3,
\end{eqnarray*}</span></p>
<p>where <span class="math notranslate nohighlight">\(m_j\)</span> is the number of sample points where <span class="math notranslate nohighlight">\(z_i = j\)</span>.</p>
<p>Only the first summation depends on <span class="math notranslate nohighlight">\(\mu_1, \sigma_1\)</span>, so the usual MLE argument for normal distribution applies, and same for <span class="math notranslate nohighlight">\(\mu_2, \sigma_2\)</span> and <span class="math notranslate nohighlight">\(\mu_3, \sigma_3\)</span>. As a result the estimators for <span class="math notranslate nohighlight">\(\mu_j, \sigma_j\)</span> are</p>
<div class="math notranslate nohighlight">
\[\mu_j = \frac{1}{m_j}\sum_{i=1}^{m_1} x_i, \qquad\sigma_j = \sqrt{\frac{1}{m_j}\sum_{i=1}^{m_j}(x_i - \mu_j)^2}, \qquad j=1, 2, 3.\]</div>
<p>To find estimator for <span class="math notranslate nohighlight">\(\phi_j\)</span> is to maximize <span class="math notranslate nohighlight">\(m_1\log\phi_1 + m_2\log\phi_2 + m_3\log\phi_3\)</span> given <span class="math notranslate nohighlight">\(\phi_1 + \phi_2 + \phi_3 = 1\)</span>. The method of Lagrange multipliers says the estimator must satisfy</p>
<div class="math notranslate nohighlight">
\[\nabla (m_1\log\phi_1 + m_2\log\phi_2 + m_3\log\phi_3) \parallel \nabla (\phi_1 + \phi_2 + \phi_3),\]</div>
<p>or equivalently <span class="math notranslate nohighlight">\((m_1/\phi_1, m_2/\phi_2, m_3/\phi_3) \parallel (1, 1, 1)\)</span>. Thus</p>
<div class="math notranslate nohighlight">
\[\phi_j = \frac{m_j}{m} \qquad j=1, 2, 3.\]</div>
<p>The AM–GM inequality would work too to optimize <span class="math notranslate nohighlight">\(\phi_1^{m_1}\phi_2^{m_2}\phi_3^{m_3}\)</span>.</p>
</div>
<div class="section" id="When-Z-Is-a-Latent-Random-Variable">
<h3>When <span class="math notranslate nohighlight">\(Z\)</span> Is a Latent Random Variable<a class="headerlink" href="#When-Z-Is-a-Latent-Random-Variable" title="Permalink to this headline">¶</a></h3>
<p>When the value of <span class="math notranslate nohighlight">\(Z\)</span> can not be observed, the EM algorithm is used for estimation where all expressions involving observation <span class="math notranslate nohighlight">\(z_i\)</span> in the above estimators, say <span class="math notranslate nohighlight">\(g(z_i)\)</span>, are replaced by a conditional expectation <span class="math notranslate nohighlight">\(E[g(z_i)|X=x_i]\)</span>. Given the current estimates of all model parameters and the observation <span class="math notranslate nohighlight">\(x_i\)</span>, the process of finding <span class="math notranslate nohighlight">\(E[g(z_i)|X=x_i]\)</span> is called the E-step. To maximize the log-likelihood function given <span class="math notranslate nohighlight">\(E[g(z_i)|X=x_i]\)</span> (as an estimate of
<span class="math notranslate nohighlight">\(g(z_i)\)</span>) is called the M-step. The EM algorithm alternates between E-steps and M-steps until convergence.</p>
<p>To derive the M-step, first rewrite the estimators derived above using indicator functions to make expressions involving <span class="math notranslate nohighlight">\(z_i\)</span> more explicit:</p>
<p><span class="math">\begin{eqnarray*}
\mu_j &=& \frac{\sum_{i=1}^{m} x_i 1_{\{z_i = j\}}}{\sum_{i=1}^{m} 1_{\{z_i = j\}}}, \\
\sigma_j &=& \sqrt{\frac{\sum_{i=1}^{m}(x_i - \mu_j)^21_{\{z_i = j\}}}{\sum_{i=1}^{m} 1_{\{z_i = j\}}}}, \\
\phi_j &=& \frac{\sum_{i=1}^{m} 1_{\{z_i = j\}}}{m}.
\end{eqnarray*}</span></p>
<p>Now replace all <span class="math notranslate nohighlight">\(1_{\{z_i = j\}}\)</span> by the conditional expectation</p>
<div class="math notranslate nohighlight">
\[E\left[1_{\{z_i = j\}}\big|X=x_i\right] = P(z_i = j|X=x_i),\]</div>
<p>which we denote by <span class="math notranslate nohighlight">\(w_i^{(j)}\)</span> for simplicity:</p>
<p><span class="math">\begin{eqnarray*}
\mu_j &=& \frac{\sum_{i=1}^{m} x_i w_i^{(j)}}{\sum_{i=1}^{m} w_i^{(j)}}, \\
\sigma_j &=& \sqrt{\frac{\sum_{i=1}^{m}(x_i - \mu_j)^2 w_i^{(j)}}{\sum_{i=1}^{m} w_i^{(j)}}}, \\
\phi_j &=& \frac{\sum_{i=1}^{m} w_i^{(j)}}{m}.
\end{eqnarray*}</span></p>
<p>These are the formulas to update estimates in the M-step.</p>
<p>The closed form formula of <span class="math notranslate nohighlight">\(w_i^{(j)}\)</span> can be obtained by Bayesian’s rule (one discrete random variable and one continuous case)</p>
<p><span class="math">\begin{eqnarray*}
w_i^{(j)} &=& P(z_i = j|X=x_i) \\
&=& \frac{p(x_i; \mu_j, \sigma_j)\phi_j}{p(x_i; \mu_1, \sigma_1)\phi_1 + p(x_i; \mu_2, \sigma_2)\phi_2 + p(x_i; \mu_3, \sigma_3)\phi_3}
\end{eqnarray*}</span></p>
<p>This is the formula to update the estimate for <span class="math notranslate nohighlight">\(w_i^{(j)}\)</span> in the E-step.</p>
</div>
</div>
<div class="section" id="Mixture-Density-Networks-(MDN)">
<h2>Mixture Density Networks (MDN)<a class="headerlink" href="#Mixture-Density-Networks-(MDN)" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>EM 學習的結果是靜態的分布。MDN 的學習結果也是 Gaussian Mixture 的參數，但是動態的，隨著 input 改變，也就是在學習 <span class="math notranslate nohighlight">\(\mu_j(\mathbf x), \sigma_j(\mathbf x), \phi_j(\mathbf x)\)</span>。</p></li>
<li><p>用下面的變換保證 <span class="math notranslate nohighlight">\(\sigma_j &gt; 0, \sum_j \phi_j = 1\)</span>，真正學習的參數是 <span class="math notranslate nohighlight">\(z^\alpha_j, z^\sigma_j, z^\mu_j\)</span>。</p>
<div class="math notranslate nohighlight">
\[\phi_j = \frac{\exp(z_j^\alpha)}{\sum_j \exp(z_j^\alpha)}, \qquad\sigma_j = \exp(z_j^\sigma), \qquad\mu_j = z_j^\mu.\]</div>
</li>
<li><p>cost function 是用 network outputs (<span class="math notranslate nohighlight">\(z^\alpha_j, z^\sigma_j, z^\mu_j\)</span>) 構造出來的分布在樣本點 <span class="math notranslate nohighlight">\(y\)</span> 值的 log-likelihood 取負，然後用 back-propagation 學習 NN 的權重。</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Notes.html" class="btn btn-neutral float-right" title="Misc" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Fun%20Math.html" class="btn btn-neutral float-left" title="Fun Math" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, beginnerSC

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>